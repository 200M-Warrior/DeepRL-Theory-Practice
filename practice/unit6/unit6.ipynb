{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/huggingface/deep-rl-class/blob/master/notebooks/unit6/unit6.ipynb","timestamp":1765099112839}],"private_outputs":true,"collapsed_sections":["tF42HvI7-gs5"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["### üéÆ Environments:\n","\n","- [Panda-Gym](https://github.com/qgallouedec/panda-gym)\n","\n","###üìö RL-Library:\n","\n","- [Stable-Baselines3](https://stable-baselines3.readthedocs.io/)"],"metadata":{"id":"QInFitfWno1Q"}},{"cell_type":"markdown","source":["## Objectives of this notebook üèÜ\n","\n","At the end of the notebook, you will:\n","\n","- Be able to use **Panda-Gym**, the environment library.\n","- Be able to **train robots using A2C**.\n","- Understand why **we need to normalize the input**.\n","- Be able to **push your trained agent and the code to the Hub** with a nice video replay and an evaluation score üî•.\n","\n","\n"],"metadata":{"id":"MoubJX20oKaQ"}},{"cell_type":"markdown","source":["## Prerequisites üèóÔ∏è\n","Before diving into the notebook, you need to:\n","\n","üî≤ üìö Study [Actor-Critic methods by reading Unit 6](https://huggingface.co/deep-rl-course/unit6/introduction) ü§ó  "],"metadata":{"id":"BTuQAUAPoa5E"}},{"cell_type":"markdown","source":["# Let's train our first robots ü§ñ"],"metadata":{"id":"iajHvVDWoo01"}},{"cell_type":"markdown","source":["To validate this hands-on for the [certification process](https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process),  you need to push your trained model to the Hub and get the following results:\n","\n","- `PandaReachDense-v3` get a result of >= -3.5.\n","\n","To find your result, go to the [leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard) and find your model, **the result = mean_reward - std of reward**\n","\n","For more information about the certification process, check this section üëâ https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process"],"metadata":{"id":"zbOENTE2os_D"}},{"cell_type":"markdown","source":["## Create a virtual display üîΩ\n","\n","During the notebook, we'll need to generate a replay video. To do so, with colab, **we need to have a virtual screen to be able to render the environment** (and thus record the frames).\n","\n","Hence the following cell will install the librairies and create and run a virtual screen üñ•"],"metadata":{"id":"bTpYcVZVMzUI"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"jV6wjQ7Be7p5"},"outputs":[],"source":["%%capture\n","!apt install python-opengl\n","!apt install ffmpeg\n","!apt install xvfb\n","!pip3 install pyvirtualdisplay"]},{"cell_type":"code","source":["# Virtual display\n","from pyvirtualdisplay import Display\n","\n","virtual_display = Display(visible=0, size=(1400, 900))\n","virtual_display.start()"],"metadata":{"id":"ww5PQH1gNLI4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Install dependencies üîΩ\n","\n","The first step is to install the dependencies, we‚Äôll install multiple ones:\n","- `gymnasium`\n","- `panda-gym`: Contains the robotics arm environments.\n","- `stable-baselines3`: The SB3 deep reinforcement learning library.\n","- `huggingface_sb3`: Additional code for Stable-baselines3 to load and upload models from the Hugging Face ü§ó Hub.\n","- `huggingface_hub`: Library allowing anyone to work with the Hub repositories.\n","\n","‚è≤ The installation can **take 10 minutes**."],"metadata":{"id":"e1obkbdJ_KnG"}},{"cell_type":"code","source":["!pip install stable-baselines3[extra]\n","!pip install gymnasium"],"metadata":{"id":"TgZUkjKYSgvn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install huggingface_sb3\n","!pip install huggingface_hub\n","!pip install panda_gym"],"metadata":{"id":"ABneW6tOSpyU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Import the packages üì¶"],"metadata":{"id":"QTep3PQQABLr"}},{"cell_type":"code","source":["import os\n","\n","import gymnasium as gym\n","import panda_gym\n","\n","from huggingface_sb3 import load_from_hub, package_to_hub\n","\n","from stable_baselines3 import A2C\n","from stable_baselines3.common.evaluation import evaluate_policy\n","from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n","from stable_baselines3.common.env_util import make_vec_env\n","\n","from huggingface_hub import notebook_login"],"metadata":{"id":"HpiB8VdnQ7Bk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## PandaReachDense-v3 ü¶æ\n","\n","The agent we're going to train is a robotic arm that needs to do controls (moving the arm and using the end-effector).\n","\n","In robotics, the *end-effector* is the device at the end of a robotic arm designed to interact with the environment.\n","\n","In `PandaReach`, the robot must place its end-effector at a target position (green ball).\n","\n","We're going to use the dense version of this environment. It means we'll get a *dense reward function* that **will provide a reward at each timestep** (the closer the agent is to completing the task, the higher the reward). Contrary to a *sparse reward function* where the environment **return a reward if and only if the task is completed**.\n","\n","Also, we're going to use the *End-effector displacement control*, it means the **action corresponds to the displacement of the end-effector**. We don't control the individual motion of each joint (joint control).\n","\n","<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit8/robotics.jpg\"  alt=\"Robotics\"/>\n","\n","\n","This way **the training will be easier**.\n","\n"],"metadata":{"id":"lfBwIS_oAVXI"}},{"cell_type":"markdown","source":["### Create the environment\n","\n","#### The environment üéÆ\n","\n","In `PandaReachDense-v3` the robotic arm must place its end-effector at a target position (green ball)."],"metadata":{"id":"frVXOrnlBerQ"}},{"cell_type":"code","source":["env_id = \"PandaReachDense-v3\"\n","\n","# Create the env\n","env = gym.make(env_id)\n","\n","# Get the state space and action space\n","s_size = env.observation_space.shape\n","a_size = env.action_space"],"metadata":{"id":"zXzAu3HYF1WD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"_____OBSERVATION SPACE_____ \\n\")\n","print(\"The State Space is: \", s_size)\n","print(\"Sample observation\", env.observation_space.sample()) # Get a random observation"],"metadata":{"id":"E-U9dexcF-FB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The observation space **is a dictionary with 3 different elements**:\n","- `achieved_goal`: (x,y,z) the current position of the end-effector.\n","- `desired_goal`: (x,y,z) the target position for the end-effector.\n","- `observation`: position (x,y,z) and velocity of the end-effector (vx, vy, vz).\n","\n","Given it's a dictionary as observation, **we will need to use a MultiInputPolicy policy instead of MlpPolicy**."],"metadata":{"id":"g_JClfElGFnF"}},{"cell_type":"code","source":["print(\"\\n _____ACTION SPACE_____ \\n\")\n","print(\"The Action Space is: \", a_size)\n","print(\"Action Space Sample\", env.action_space.sample()) # Take a random action"],"metadata":{"id":"ib1Kxy4AF-FC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The action space is a vector with 3 values:\n","- Control x, y, z movement"],"metadata":{"id":"5MHTHEHZS4yp"}},{"cell_type":"markdown","source":["### Normalize observation and rewards"],"metadata":{"id":"S5sXcg469ysB"}},{"cell_type":"markdown","source":["A good practice in reinforcement learning is to [normalize input features](https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html).\n","\n","For that purpose, there is a wrapper that will compute a running average and standard deviation of input features.\n","\n","We also normalize rewards with this same wrapper by adding `norm_reward = True`\n","\n","[You should check the documentation to fill this cell](https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#vecnormalize)"],"metadata":{"id":"1ZyX6qf3Zva9"}},{"cell_type":"code","source":["env = make_vec_env(env_id, n_envs=4)\n","\n","# Adding this wrapper to normalize the observation and the reward\n","env = VecNormalize(env, norm_obs=True, norm_reward=True, clip_obs=10.)"],"metadata":{"id":"1RsDtHHAQ9Ie"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Solution"],"metadata":{"id":"tF42HvI7-gs5"}},{"cell_type":"code","source":["env = make_vec_env(env_id, n_envs=4)\n","\n","env = VecNormalize(env, norm_obs=True, norm_reward=True, clip_obs=10.)"],"metadata":{"id":"2O67mqgC-hol"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Create the A2C Model ü§ñ\n","\n","For more information about A2C implementation with StableBaselines3 check: https://stable-baselines3.readthedocs.io/en/master/modules/a2c.html#notes\n","\n","To find the best parameters I checked the [official trained agents by Stable-Baselines3 team](https://huggingface.co/sb3)."],"metadata":{"id":"4JmEVU6z1ZA-"}},{"cell_type":"code","source":["model = A2C(policy = \"MultiInputPolicy\",\n","            env = env,\n","            verbose=1)"],"metadata":{"id":"vR3T4qFt164I"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Solution"],"metadata":{"id":"nWAuOOLh-oQf"}},{"cell_type":"code","source":["model = A2C(policy = \"MultiInputPolicy\",\n","            env = env,\n","            verbose=1)"],"metadata":{"id":"FKFLY54T-pU1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Train the A2C agent üèÉ\n","- Let's train our agent for 1,000,000 timesteps, don't forget to use GPU on Colab. It will take approximately ~25-40min"],"metadata":{"id":"opyK3mpJ1-m9"}},{"cell_type":"code","source":["model.learn(1_000_000)"],"metadata":{"id":"4TuGHZD7RF1G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Save the model and  VecNormalize statistics when saving the agent\n","model.save(\"a2c-PandaReachDense-v3\")\n","env.save(\"vec_normalize.pkl\")"],"metadata":{"id":"MfYtjj19cKFr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Evaluate the agent üìà\n","- Now that's our  agent is trained, we need to **check its performance**.\n","- Stable-Baselines3 provides a method to do that: `evaluate_policy`"],"metadata":{"id":"01M9GCd32Ig-"}},{"cell_type":"code","source":["from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n","\n","# Load the saved statistics\n","eval_env = DummyVecEnv([lambda: gym.make(\"PandaReachDense-v3\")])\n","eval_env = VecNormalize.load(\"vec_normalize.pkl\", eval_env)\n","\n","# We need to override the render_mode\n","eval_env.render_mode = \"rgb_array\"\n","\n","#  do not update them at test time\n","eval_env.training = False\n","# reward normalization is not needed at test time\n","eval_env.norm_reward = False\n","\n","# Load the agent\n","model = A2C.load(\"a2c-PandaReachDense-v3\")\n","\n","mean_reward, std_reward = evaluate_policy(model, eval_env)\n","\n","print(f\"Mean reward = {mean_reward:.2f} +/- {std_reward:.2f}\")"],"metadata":{"id":"liirTVoDkHq3"},"execution_count":null,"outputs":[]}]}